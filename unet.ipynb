{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "from dataloader import PASTIS_Dataset\n",
    "from collate import pad_collate\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порядок комментариев к коду: китайский / русский / английский."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双卷积 / двойная свертка / double convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    # (convolution => [BN] => ReLU) * 2\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力 / механизм внимания / attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    # 注意力模块-FPNUNet / Блок внимания для FPNUNet(Пока не используется) / Attention Block for FPNUNet\n",
    "\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "         # 在相加前上采样g以匹配x的尺寸 / Перед сложением g повышается до размеров x / Upsample g to match the size of x before adding\n",
    "        g1 = F.interpolate(g1, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        \n",
    "        return x * psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "金字塔U-NET/ Модель FPN-UNet / FPN-UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNUNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Conv2d(300, 64, kernel_size=1)\n",
    "        # 编码器 / Кодировщик / Encoder\n",
    "        \"\"\"\n",
    "        使用了4层编码器 / Используется 4 слоя кодировщика / 4 layers of encoder\n",
    "        正则化设定0.5 / Настройка регуляризации 0.5 / Regularization setting 0.5\n",
    "        \"\"\"\n",
    "        self.encoder1 = DoubleConv(64, 64)\n",
    "        self.droupout1 = nn.Dropout(dropout_rate)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.droupout2 = nn.Dropout(dropout_rate)\n",
    "        self.encoder3 = DoubleConv(128, 256)\n",
    "        self.droupout3 = nn.Dropout(dropout_rate)\n",
    "        self.encoder4 = DoubleConv(256, 512)\n",
    "        self.droupout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 解码器 / Декодер / Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(128, 64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv(128, 64)\n",
    "        # FPN桥 / Мост FPN / FPN Bridge\n",
    "        self.fpn_bridge = DoubleConv(512, 512)\n",
    "        self.fpn_bridge_dropout = nn.Dropout(dropout_rate)\n",
    "        # 最终分类器 / Финальный классификатор / Final classifier\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        # 注意力层（可选） / Слой внимания (опционально) / Attention Layer (Optional)\n",
    "        self.attention1 = AttentionBlock(F_g=512, F_l=512, F_int=256)\n",
    "        self.attention2 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "        self.attention3 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "        self.attention4 = AttentionBlock(F_g=64, F_l=64, F_int=32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        # 编码 / Путь кодировщика / Encoder path\n",
    "        x1 = self.droupout1(self.encoder1(x))\n",
    "        x2 = self.droupout2(self.encoder2(x1))\n",
    "        x3 = self.droupout3(self.encoder3(x2))\n",
    "        x4 = self.droupout4(self.encoder4(x3))\n",
    "       \n",
    "        # FPN桥 / Мост FPN / FPN Bridge\n",
    "        \"\"\"\n",
    "        FPN桥连接了编码器和解码器 / Мост FPN соединяет кодировщик и декодер / FPN Bridge connects the encoder and decoder\n",
    "        FPN桥的作用额外增加了一层正则化 / Роль моста FPN дополнительно увеличивает уровень регуляризации / The role of FPN bridge additionally increases the level of regularization\n",
    "        \"\"\"\n",
    "        x_bridge = self.fpn_bridge_dropout(self.fpn_bridge(x4))\n",
    "        \n",
    "        # 解码 / Путь декодера / Decoder path\n",
    "        x = self.upconv4(x_bridge)\n",
    "        # 使用双线性插值上采样 / Восстановление с использованием билинейной интерполяции / Upsampling using bilinear interpolation\n",
    "        x = F.interpolate(x, size=x3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat((x, x3), dim=1)\n",
    "        # x = self.attention1(g=x, x=x4)\n",
    "        x = self.decoder4(x)\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = F.interpolate(x, size=x2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat((x, x2), dim=1)\n",
    "        # x = self.attention2(g=x, x=x3)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = F.interpolate(x, size=x1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x1_upsampled = F.interpolate(x1, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat((x, x1_upsampled), dim=1) \n",
    "        x = self.decoder1(x)\n",
    "\n",
    "        x = F.interpolate(x, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "        # Final classification\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU / Определите, можно ли использовать Cuda / To see if Cuda can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU support enabled.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU support enabled.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集加载 / Загрузка набора данных / loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3035178815.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_size = int(0. 8 * len(subset_dataset))\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# 获取并处理数据集 / Получение и обработка набора данных / Getting and processing the dataset\n",
    "path_to_dataset = '/content/drive/MyDrive/Colab Notebooks/data/PASTIS'\n",
    "dataset = PASTIS_Dataset(path_to_dataset, norm=True, target='semantic') # 使用语义分割标签 / Использование меток семантической сегментации / Using semantic segmentation labels\n",
    "subset_indices = torch.randperm(len(dataset))[:1500].tolist()\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "# 划分训练集和验证集 / Разделение на обучающий и проверочный наборы / Splitting into training and validation sets\n",
    "train_size = int(0.8 * len(subset_dataset))\n",
    "valid_size = len(subset_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(subset_dataset, [train_size, valid_size])\n",
    "\n",
    "# 创建 DataLoader / Создание DataLoader / Creating DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, collate_fn=pad_collate, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, collate_fn=pad_collate)\n",
    "\n",
    "# 类别数 / Количество классов / Number of classes\n",
    "num_classes = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "早停 / Ранняя остановка / Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "patience: 训练过程中没有改进的次数。\n",
    "patience: Количество раз, когда обучение не улучшается.\n",
    "patience: The number of times the training does not improve.\n",
    "\n",
    "min_delta: 被认为是改进的最小变化量。\n",
    "min_delta: Минимальное изменение, которое считается улучшением.\n",
    "min_delta: The minimum change that is considered an improvement.\n",
    "\"\"\"\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型 / обучение модели / model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Software\\Work\\Anaconda\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55b6105ab674f798005f1ed571a59aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 1.6537331926822663, Validation Loss: 1.2932375892003378, Overall Accuracy: 0.6040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1733292417414f538e26752fd70696e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Training Loss: 1.342187801003456, Validation Loss: 1.1917772380510967, Overall Accuracy: 0.6375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9030bcfe6e2441ba40ed2d394d11eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Training Loss: 1.2566123658418655, Validation Loss: 1.1145870892206828, Overall Accuracy: 0.6558\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee303d803a00458ca1401fedd47b6cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Training Loss: 1.2027760229508082, Validation Loss: 1.123127597173055, Overall Accuracy: 0.6425\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4dcb6125914be9a84e4cf986d9b418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Training Loss: 1.1574705028533936, Validation Loss: 1.04104363600413, Overall Accuracy: 0.6640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0ebcae322f4ed8802fad32c0455e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Training Loss: 1.116131874124209, Validation Loss: 1.0366944082578022, Overall Accuracy: 0.6710\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62954fa8b1b8443fbc03599c17ba7814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Training Loss: 1.0964794039726258, Validation Loss: 1.0506828173001608, Overall Accuracy: 0.6597\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7477d5cde54d1f9519885fc9fb3613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Training Loss: 1.0720294284820557, Validation Loss: 0.9907202394803365, Overall Accuracy: 0.6769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7c31b838a146d584ba3a79ee809782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Training Loss: 1.0548770779371262, Validation Loss: 0.9840745250384013, Overall Accuracy: 0.6798\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183a293f06c84cfaa9a1f61f5388c37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Training Loss: 1.045912851492564, Validation Loss: 0.9822000702222188, Overall Accuracy: 0.6813\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed28a3083f442b7a33c0eb045977671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Training Loss: 1.031415196855863, Validation Loss: 0.9667937167485555, Overall Accuracy: 0.6845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe11ed96aa9e4918a154e95ddfb38965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Training Loss: 1.0193106210231782, Validation Loss: 0.9627852114041646, Overall Accuracy: 0.6863\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f8ace97951440aad821cb34e524d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Training Loss: 1.0043124649922053, Validation Loss: 0.9636413995424906, Overall Accuracy: 0.6864\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec44059622c4ba58e5cff6d0733ac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Training Loss: 1.0004757742087047, Validation Loss: 0.9452791961034139, Overall Accuracy: 0.6920\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和优化器 / Инициализация модели и оптимизатора / Initializing the model and optimizer\n",
    "model = FPNUNet(num_classes=num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "\n",
    "# 初始化学习率调度器 / Инициализация планировщика скорости обучения / Initializing the learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# 训练循环 / Цикл обучения / Training loop\n",
    "epochs = 30 # 训练周期 / Эпохи обучения / Training epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, batch_data in tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n",
    "        ((inputs_dict, dates), targets) = batch_data\n",
    "        # 将三十个时间点合并 / Объединение тридцати временных точек / Combining thirty time points\n",
    "        inputs_combined = torch.cat([inputs_dict['S2'][:, i, :, :, :] for i in range(30)], dim=1).to(device) \n",
    "        targets = targets.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs_combined)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()  # 累加训练损失 / Накопление потерь обучения / Accumulating training loss\n",
    "\n",
    "    train_loss /= len(train_loader)  # 计算平均训练损失 / Вычисление средних потерь обучения / Calculating average training loss\n",
    "\n",
    "    # 验证阶段 / Валидация / Validation phase\n",
    "    model.eval()  # 设置模型为评估模式 / Установка модели в режим оценки / Setting the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    with torch.no_grad():  # 在这个阶段不计算梯度 / На этом этапе градиенты не вычисляются / Gradients are not calculated at this stage\n",
    "        for batch_data in valid_loader:\n",
    "            ((inputs_dict, dates), targets) = batch_data\n",
    "            inputs_combined = torch.cat([inputs_dict['S2'][:, i, :, :, :] for i in range(30)], dim=1).to(device)\n",
    "            targets = targets.to(device).long()\n",
    "\n",
    "            outputs = model(inputs_combined)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()  # 累加验证损失 / Накопление потерь валидации / Accumulating validation loss\n",
    "            # 计算准确率 /  Вычисление точности / Calculating accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # 获取最大概率的预测结果 / Получение предсказанных результатов с максимальной вероятностью / Getting predicted results with maximum probability\n",
    "            correct_pixels += (predicted == targets).sum().item()  # 累加正确预测的像素数 / Накопление количества правильно предсказанных пикселей / Accumulating the number of correctly predicted pixels\n",
    "            total_pixels += targets.nelement()  # 累加总像素数 / Накопление общего количества пикселей / Accumulating the total number of pixels\n",
    "\n",
    "    val_loss /= len(valid_loader)  # 计算平均验证损失 / Вычисление средних потерь валидации / Calculating average validation loss\n",
    "    overall_accuracy = correct_pixels / total_pixels  # 计算总体准确率 / Вычисление общей точности / Calculating overall accuracy\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    # 在这里调用学习率调度器，基于验证损失 / Вызов планировщика скорости обучения на основе потерь валидации / Calling the learning rate scheduler here, based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 检查是否需要早停 / Проверка на необходимость досрочной остановки / Checking if early stopping is needed\n",
    "    early_stopping(loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算mIoU / Рассчитать mIoU / Calculate mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(predicted, target, num_classes):\n",
    "    \"\"\"\n",
    "    计算平均IoU，对每个类别计算IoU，然后取平均值。\n",
    "    IoU = TP / (TP + FP + FN) 指的是交集与并集的比值。\n",
    "    Вычисление среднего IoU, вычисление IoU для каждого класса, а затем взятие среднего значения.\n",
    "    IoU = TP / (TP + FP + FN) отношение пересечения к объединению.\n",
    "    Calculate mean IoU, calculate IoU for each class, then take the average.\n",
    "    IoU = TP / (TP + FP + FN) the ratio of intersection to union.\n",
    "    TP: True Positive, FP: False Positive, FN: False Negative\n",
    "    \"\"\"\n",
    "    iou_list = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = predicted == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "        if union == 0:\n",
    "            # 避免除以0 / Избегание деления на 0 / Avoiding division by zero\n",
    "            iou_list.append(float('nan'))  # 该类别未出现在预测和目标中 / Этот класс не появляется в прогнозе и цели / This class does not appear in the prediction and target\n",
    "        else:\n",
    "            iou_list.append(intersection / union)\n",
    "    # 忽略nan值计算平均IoU / Игнорирование значений nan при вычислении среднего IoU / Ignoring nan values when calculating mean IoU\n",
    "    iou_list = [x for x in iou_list if not np.isnan(x)]\n",
    "    mean_iou = sum(iou_list) / len(iou_list) if iou_list else float('nan')\n",
    "    return mean_iou\n",
    "\n",
    "# 模型验证和计算Mean IoU / Проверка модели и вычисление среднего IoU / Model validation and calculating Mean IoU\n",
    "def validate_and_calculate_iou(model, loader, device, num_classes):\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    with torch.no_grad():\n",
    "        for ((inputs_dict, dates), targets) in loader:\n",
    "            inputs_combined = torch.cat([inputs_dict['S2'][:, i, :, :, :] for i in range(30)], dim=1).to(device)\n",
    "            targets = targets.to(device).long()\n",
    "\n",
    "            outputs = model(inputs_combined)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_iou += calculate_iou(predicted, targets, num_classes)\n",
    "            # 计算准确率 / Вычисление точности / Calculating accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # 获取最大概率的预测结果 / Получение предсказанных результатов с максимальной вероятностью / Getting predicted results with maximum probability\n",
    "            correct_pixels += (predicted == targets).sum().item()  # 累加正确预测的像素数 / Накопление количества правильно предсказанных пикселей / Accumulating the number of correctly predicted pixels\n",
    "            total_pixels += targets.nelement()  # 累加总像素数 / Накопление общего количества пикселей / Accumulating the total number of pixels\n",
    "\n",
    "    mean_iou = total_iou / len(loader)\n",
    "    overall_accuracy = correct_pixels / total_pixels  # 计算总体准确率 / Вычисление общей точности / Calculating overall accuracy\n",
    "    print(f\"Mean IoU on validation set: {mean_iou}, Overall Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证 / Проверить модель / Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU on validation set: 0.26635117980806505, Overall Accuracy: 0.6920\n",
      "Total trainable parameters: 12959232\n"
     ]
    }
   ],
   "source": [
    "# 调用验证函数 / Вызов функции валидации / Calling the validation function\n",
    "validate_and_calculate_iou(model, valid_loader, device, num_classes)\n",
    "\n",
    "# 计算模型参数数量 / Вычисление количества параметров модели / Calculating the number of model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total trainable parameters: {total_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
