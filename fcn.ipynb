{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"wAAVpaZ4A4y_"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split, Dataset\n","import numpy as np\n","from dataloader import PASTIS_Dataset\n","from collate import pad_collate\n","import torch.nn.functional as F\n","from tqdm.auto import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from torch.cuda.amp import autocast, GradScaler\n","from torchvision import models\n","from torchvision.models.vgg import VGG\n","import os\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap, BoundaryNorm\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report"]},{"cell_type":"markdown","metadata":{},"source":["数据集加载 / Загрузка набора данных / loading dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"n6Adl8P0A4zA"},"outputs":[],"source":["class DynamicTimePointDataset(Dataset):\n","    def __init__(self, dataset, indices):\n","        self.dataset = dataset\n","        self.indices = indices\n","        self.index_mapping = self._create_index_mapping()\n","\n","    def _create_index_mapping(self):\n","        mapping = []\n","        for idx in self.indices:\n","            (data, dates), target = self.dataset[idx]\n","            s2_data = data['S2']\n","            num_time_points = s2_data.shape[0]\n","            for time_point in range(num_time_points):\n","                mapping.append((idx, time_point))\n","        return mapping\n","\n","    def __len__(self):\n","        return len(self.index_mapping)\n","\n","    def __getitem__(self, idx):\n","        patch_idx, time_point_idx = self.index_mapping[idx]\n","        (data, dates), target = self.dataset[patch_idx]\n","        s2_data = data['S2']\n","        time_point_data = s2_data[time_point_idx].unsqueeze(0)  \n","        return time_point_data, target"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14154,"status":"ok","timestamp":1711146987207,"user":{"displayName":"Nigel Ning","userId":"05037057044039723428"},"user_tz":-180},"id":"iqcFtZpzA4zB","outputId":"83642d6d-d260-4e87-f889-8d47da10aba6"},"outputs":[],"source":["# 获取并处理数据集 / Получение и обработка набора данных / Getting and processing the dataset\n","path_to_dataset = 'E:/Research/Newdata/PASTIS'\n","dataset = PASTIS_Dataset(path_to_dataset, norm=True, target='semantic') # 使用语义分割标签 / Использование меток семантической сегментации / Using semantic segmentation labels\n","\n","subset_indices = torch.randperm(len(dataset))[:1500].tolist()\n","dynamic_dataset = DynamicTimePointDataset(dataset, subset_indices)\n","total_samples = len(dynamic_dataset)\n","print(f\"Total number of data samples: {total_samples}\")\n","\n","# 划分训练集和验证集 / Разделение на обучающий и проверочный наборы / Splitting into training and validation sets\n","train_size = int(0.8 * len(dynamic_dataset))\n","valid_size = len(dynamic_dataset) - train_size\n","train_dataset, valid_dataset = random_split(dynamic_dataset, [train_size, valid_size])\n","\n","valid_data = [(data, label) for data, label in DataLoader(valid_dataset, batch_size=1)]\n","torch.save(valid_data, 'valid_data.pt')\n","\n","# 创建 DataLoader / Создание DataLoader / Creating DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=pad_collate, shuffle=True, pin_memory=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=16, collate_fn=pad_collate, pin_memory=True)\n","\n","# 类别数 / Количество классов / Number of classes\n","num_classes = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1711146987207,"user":{"displayName":"Nigel Ning","userId":"05037057044039723428"},"user_tz":-180},"id":"MyN4OsF1A4zB","outputId":"20ea4ce1-ce99-4f46-9f3a-27fab9a2b544"},"outputs":[],"source":["if torch.cuda.is_available():\n","    print(\"CUDA is available. GPU is used now.\")\n","    device = torch.device(\"cuda\")\n","else:\n","    print(\"CUDA is not available. Using CPU.\")\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["早停 / Ранняя остановка / Early stopping"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"oXXvVdIrA4zB"},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_loss = float('inf')\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss):\n","        if self.best_loss - val_loss > self.min_delta:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True"]},{"cell_type":"markdown","metadata":{},"source":["FCN"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Dv1XN519A4zB"},"outputs":[],"source":["class FCN10Channel(nn.Module):\n","    def __init__(self, num_channels=10, num_classes=20):\n","        super(FCN10Channel, self).__init__()\n","        vgg = models.vgg16(pretrained=True)\n","        features = list(vgg.features.children())\n","        features[0] = nn.Conv2d(num_channels, 64, kernel_size=3, padding=1)\n","        self.features = nn.Sequential(*features)\n","        \n","        self.fcn = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, num_classes, kernel_size=1)\n","        )\n","        \n","        self.upsample = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, padding=16)\n","\n","    def forward(self, x):\n","        x = self.features(x)  \n","        x = self.fcn(x)      \n","        x = self.upsample(x)  \n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["训练模型 / обучение модели / model training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179,"referenced_widgets":["ee4149431bf94221b2752786de06e872","baabe053691e4d61b71942526720ac1c","b2e39beddafb43bcbea678c86478f4d8","38f95f05972a43b6a7c6f92c4db58c27","7a1c17a30d054d69884c5c148d7314c8","c17c53e718c34af78e37de04c171434a","15ba63160f7843279d49463ee477fb18","69c0a4486e0c4664921b0a715ae7b09e","65c7a07f738d497eb6af4177eef3cc4e","53a49dad5c444de2929f3b9a4e4bdbd0","2ec17e5370b7442f9f34e6f7db1e13b2"]},"id":"pxjxu_AFA4zC","outputId":"ece2b471-af78-422a-a5e2-2caf0e1484b3"},"outputs":[],"source":["model = FCN10Channel(num_channels=10, num_classes=20).to(device)\n","model.load_state_dict(torch.load('best_fcn_0556.pth'))\n","model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=7e-7)\n","\n","\n","scaler = GradScaler()\n","criterion = nn.CrossEntropyLoss()\n","# 初始化学习率调度器 / Инициализация планировщика скорости обучения / Initializing the learning rate scheduler\n","scheduler = ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.1, verbose=True, min_lr=1e-6)\n","early_stopping = EarlyStopping(patience=5, min_delta=0.0001)\n","\n","writer = SummaryWriter()\n","\n","def save_model(model, path):\n","    torch.save(model.state_dict(), path)\n","\n","# 记录训练过程 / Запись процесса обучения / Recording the training process\n","train_losses = []\n","val_losses = []\n","overall_accuracies = []\n","precision_scores = []\n","f1_scores = []\n","recall_scores = []\n","best_val_loss = float('inf') \n","\n","# 训练循环 / Цикл обучения / Training loop\n","epochs = 50 # 训练周期 / Эпохи обучения / Training epochs\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for batch_idx, batch_data in tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n","        (inputs, targets) = batch_data\n","        targets = targets.to(device).long()\n","        optimizer.zero_grad()\n","\n","        with autocast():\n","            inputs = torch.squeeze(inputs, dim=1).to(device)  \n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        train_loss += loss.item()  # 累加训练损失 / Накопление потерь обучения / Accumulating training loss\n","\n","    train_loss /= len(train_loader)  # 计算平均训练损失 / Вычисление средних потерь обучения / Calculating average training loss\n","\n","    if (epoch +1) % 2 == 0:\n","        # 验证阶段 / Валидация / Validation phase\n","        model.eval()  # 设置模型为评估模式 / Установка модели в режим оценки / Setting the model to evaluation mode\n","        val_loss = 0.0\n","        correct_pixels = 0\n","        total_pixels = 0\n","        all_predictions = []\n","        all_targets = []\n","        with torch.no_grad():  # 在这个阶段不计算梯度 / На этом этапе градиенты не вычисляются / Gradients are not calculated at this stage\n","            for batch_data in valid_loader:\n","                (inputs, targets) = batch_data\n","                targets = targets.to(device).long()\n","                inputs = torch.squeeze(inputs, dim=1).to(device)  \n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","\n","                val_loss += loss.item()  # 累加验证损失 / Накопление потерь валидации / Accumulating validation loss\n","                # 计算准确率 /  Вычисление точности / Calculating accuracy\n","                _, predicted = torch.max(outputs, 1)  # 获取最大概率的预测结果 / Получение предсказанных результатов с максимальной вероятностью / Getting predicted results with maximum probability\n","                correct_pixels += (predicted == targets).sum().item()  # 累加正确预测的像素数 / Накопление количества правильно предсказанных пикселей / Accumulating the number of correctly predicted pixels\n","                total_pixels += targets.nelement()  # 累加总像素数 / Накопление общего количества пикселей / Accumulating the total number of pixels\n","                all_predictions.append(predicted.cpu().numpy())\n","                all_targets.append(targets.cpu().numpy())\n","\n","        all_predictions_flattened = np.concatenate(all_predictions).reshape(-1)\n","        all_targets_flattened = np.concatenate(all_targets).reshape(-1)\n","\n","        val_loss /= len(valid_loader)  # 计算平均验证损失 / Вычисление средних потерь валидации / Calculating average validation loss\n","        overall_accuracy = correct_pixels / total_pixels  # 计算总体准确率 / Вычисление общей точности / Calculating overall accuracy\n","        precision = precision_score(all_targets_flattened, all_predictions_flattened, average='macro', zero_division=0)  # 计算精确率 / Вычисление точности / Calculating precision\n","        recall = recall_score(all_targets_flattened, all_predictions_flattened, average='macro', zero_division=0)  # 计算召回率 / Вычисление полноты / Calculating recall\n","        f1 = f1_score(all_targets_flattened, all_predictions_flattened, average='macro', zero_division=0)  # 计算F1 / Вычисление F1 / Calculating F1\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Overall Accuracy: {overall_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n","\n","        train_losses.append(train_loss)  # 记录训练损失 / Запись потерь обучения / Recording training loss\n","        val_losses.append(val_loss)  # 记录验证损失 / Запись потерь валидации / Recording validation loss\n","        overall_accuracies.append(overall_accuracy)  # 记录总体准确率 / Запись общей точности / Recording overall accuracy\n","        precision_scores.append(precision)  # 记录精确率 / Запись точности / Recording precision\n","        recall_scores.append(recall)  # 记录召回率 / Запись полноты / Recording recall\n","        f1_scores.append(f1)  # 记录F1 / Запись F1 / Recording F1\n","\n","        # 记录到TensorBoard / Запись в TensorBoard / Recording to TensorBoard\n","        for name, param in model.named_parameters():\n","            writer.add_histogram(f'Weights/{name}', param, epoch)\n","            if param.grad is not None:\n","                writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n","        writer.add_scalar('Loss/train', train_loss, epoch)\n","        writer.add_scalar('Loss/val', val_loss, epoch)\n","        writer.add_scalar('Accuracy/overall', overall_accuracy, epoch)\n","        writer.add_scalar('Precision', precision, epoch)\n","        writer.add_scalar('Recall', recall, epoch)\n","        writer.add_scalar('F1', f1, epoch)\n","        writer.add_scalar('Learning rate', optimizer.param_groups[0]['lr'], epoch)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            save_model(model, 'best_model_state_dict.pth')\n","            print(f\"Model saved at Epoch {epoch+1}: Improved validation loss to {best_val_loss:.4f}\")\n","\n","        # 在这里调用学习率调度器，基于验证损失 / Вызов планировщика скорости обучения на основе потерь валидации / Calling the learning rate scheduler here, based on validation loss\n","        scheduler.step(val_loss)\n","\n","        # 检查是否需要早停 / Проверка на необходимость досрочной остановки / Checking if early stopping is needed\n","        early_stopping(val_loss)\n","        if early_stopping.early_stop:\n","            print(\"Early stopping triggered.\")\n","            break\n","writer.close()"]},{"cell_type":"markdown","metadata":{},"source":["计算mIoU / Рассчитать mIoU / Calculate mIoU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzXPE1tcA4zC"},"outputs":[],"source":["def calculate_iou(predicted, target, num_classes):\n","    iou_list = []\n","    for cls in range(num_classes):\n","        pred_inds = predicted == cls\n","        target_inds = target == cls\n","        intersection = (pred_inds & target_inds).sum().item()\n","        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n","        if union == 0:\n","            # 避免除以0 / Избегание деления на 0 / Avoiding division by zero\n","            iou_list.append(float('nan'))  # 该类别未出现在预测和目标中 / Этот класс не появляется в прогнозе и цели / This class does not appear in the prediction and target\n","        else:\n","            iou_list.append(intersection / union)\n","    # 忽略nan值计算平均IoU / Игнорирование значений nan при вычислении среднего IoU / Ignoring nan values when calculating mean IoU\n","    iou_list = [x for x in iou_list if not np.isnan(x)]\n","    mean_iou = sum(iou_list) / len(iou_list) if iou_list else float('nan')\n","    return mean_iou\n","\n","# 模型验证和计算Mean IoU / Проверка модели и вычисление среднего IoU / Model validation and calculating Mean IoU\n","def validate_and_calculate_iou(model, loader, device, num_classes):\n","    model.eval()\n","    total_iou = 0.0\n","    correct_pixels = 0\n","    total_pixels = 0\n","    all_targets = []\n","    all_predictions = []\n","    with torch.no_grad():\n","        for (inputs, targets) in loader:\n","            targets = targets.to(device).long()\n","            inputs = torch.squeeze(inputs, dim=1).to(device) \n","\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total_iou += calculate_iou(predicted, targets, num_classes)\n","            # 计算准确率 / Вычисление точности / Calculating accuracy\n","            _, predicted = torch.max(outputs, 1)  # 获取最大概率的预测结果 / Получение предсказанных результатов с максимальной вероятностью / Getting predicted results with maximum probability\n","            correct_pixels += (predicted == targets).sum().item()  # 累加正确预测的像素数 / Накопление количества правильно предсказанных пикселей / Accumulating the number of correctly predicted pixels\n","            total_pixels += targets.nelement()  # 累加总像素数 / Накопление общего количества пикселей / Accumulating the total number of pixels\n","            all_predictions.append(predicted.cpu().numpy())\n","            all_targets.append(targets.cpu().numpy())\n","\n","    # 扁平化预测和目标张量 / Плоскость тензоров предсказаний и целей / Flattening the prediction and target tensors\n","    all_predictions_flattened = np.concatenate(all_predictions).reshape(-1)\n","    all_targets_flattened = np.concatenate(all_targets).reshape(-1)\n","\n","    mean_iou = total_iou / len(loader)\n","    overall_accuracy = correct_pixels / total_pixels  # 计算总体准确率 / Вычисление общей точности / Calculating overall accuracy\n","    precision = precision_score(all_targets_flattened, all_predictions_flattened, average='macro', zero_division=0)  # 计算精确率 / Вычисление точности / Calculating precision\n","    recall = recall_score(all_targets_flattened, all_predictions_flattened, average='macro', zero_division=0)  # 计算召回率 / Вычисление полноты / Calculating recall\n","    f1 = f1_score(all_targets_flattened, all_predictions_flattened, average='macro', zero_division=0)  # 计算Fs1 / Вычисление F1 / Calculating F1\n","    print(f\"Mean IoU on validation set: {mean_iou}, Overall Accuracy: {overall_accuracy:.4f}\", f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["验证 / Проверить модель / Validate model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR_vKlaKA4zC"},"outputs":[],"source":["# 调用验证函数 / Вызов функции валидации / Calling the validation function\n","validate_and_calculate_iou(model, valid_loader, device, num_classes)\n","\n","# 计算模型参数数量 / Вычисление количества параметров модели / Calculating the number of model parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total trainable parameters: {total_params}\")"]},{"cell_type":"markdown","metadata":{},"source":["可视化 / Визуализация / Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3W7JO1j7A4zC"},"outputs":[],"source":["plt.figure(figsize=(12, 5))\n","plt.subplot(1, 3, 1)\n","plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n","plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Loss Over Epochs')\n","\n","plt.subplot(1, 3, 2)\n","plt.plot(range(1, len(overall_accuracies)+1), overall_accuracies, label='Overall Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Overall Accuracy')\n","plt.legend()\n","plt.title('Overall Over Epochs')\n","\n","plt.subplot(1, 3, 3)\n","plt.plot(range(1, len(precision_scores)+1), precision_scores, label='Precision')\n","plt.xlabel('Epochs')\n","plt.ylabel('Precision')\n","plt.legend()\n","plt.title('Precision Over Epochs')\n","\n","plt.subplot(2, 2, 1)\n","plt.plot(range(1, len(recall_scores)+1), recall_scores, label='Recall')\n","plt.xlabel('Epochs')\n","plt.ylabel('Recall')\n","plt.legend()\n","plt.title('Recall Over Epochs')\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(range(1, len(f1_scores)+1), f1_scores, label='F1')\n","plt.xlabel('Epochs')\n","plt.ylabel('F1')\n","plt.legend()\n","plt.title('F1 Over Epochs')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","conf_mat = confusion_matrix(all_targets_flattened, all_predictions_flattened)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","print(classification_report(all_targets_flattened, all_predictions_flattened))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_data = torch.load('valid_data.pth')\n","\n","class SimpleDataset(torch.utils.data.Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","valid_dataset = SimpleDataset(valid_data)\n","valida_loader = DataLoader(valid_dataset, batch_size=16, collate_fn=pad_collate, pin_memory=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = FCN10Channel(num_channels=10, num_classes=20).to(device)\n","model.load_state_dict(torch.load('best_fcn_0556.pth'))\n","model.to(device)\n","\n","def visualize_overlay(images, labels, predictions, alpha=0.5, num_images=3):\n","    colors = [\n","    '#FFFFFF',  # white for background class 0\n","    '#E6194B',  # red for class 1\n","    '#3CB44B',  # green for class 2\n","    '#FFE119',  # yellow for class 3\n","    '#4363D8',  # blue for class 4\n","    '#F58231',  # orange for class 5\n","    '#911EB4',  # purple for class 6\n","    '#46F0F0',  # cyan-blue for class 7\n","    '#F032E6',  # pink for class 8\n","    '#BCF60C',  # lime Green for class 9\n","    '#FABEBE',  # light pink for class 10\n","    '#008080',  # light cyan-blue for class 11\n","    '#E6BEFF',  # mauve for class 12\n","    '#9A6324',  # brown for class 13\n","    '#FFFAC8',  # cream for class 14\n","    '#800000',  # maroon for class 15\n","    '#AAFFC3',  # Mint Green for class 16\n","    '#808000',  # Olive Green for class 17\n","    '#FFD8B1',  # coral for class 18\n","    '#000075',  # Dark Blue for class 19\n","    ]\n","\n","    cmap_custom = ListedColormap(colors)\n","    norm = BoundaryNorm(np.arange(len(colors) + 1), cmap_custom.N)  \n","\n","    fig, axs = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n","    for i in range(num_images):\n","        if num_images == 1:\n","            ax1, ax2, ax3 = axs\n","        else:\n","            ax1, ax2, ax3 = axs[i]\n","\n","        img_display = images[i][[1, 2, 3]].permute(1, 2, 0).cpu().numpy()\n","        img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min())\n","\n","        ax1.imshow(img_display)\n","        ax1.set_title(\"Original Image - RGB\")\n","        ax1.axis('off')\n","\n","        ax2.imshow(img_display)\n","        ax2.imshow(labels[i].cpu().numpy(), cmap=cmap_custom, norm=norm, alpha=alpha)  \n","        ax2.set_title(\"True Label Overlay\")\n","        ax2.axis('off')\n","\n","        ax3.imshow(img_display) \n","        ax3.imshow(predictions[i].cpu().numpy(), cmap=cmap_custom, norm=norm, alpha=alpha) \n","        ax3.set_title(\"Prediction Overlay\")\n","        ax3.axis('off')\n","\n","    plt.show()\n","\n","\n","model.eval()\n","with torch.no_grad():\n","    for (inputs, targets) in valida_loader:\n","        print(inputs.shape)\n","        targets = torch.squeeze(targets, dim=1)\n","        targets = targets.to(device).long()\n","        inputs = torch.squeeze(inputs, dim=1).to(device) \n","        inputs = torch.squeeze(inputs, dim=1).to(device)\n","\n","        print(inputs.shape, targets.shape)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","\n","        visualize_overlay(inputs, targets, predicted, num_images=10)\n","        break  "]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"15ba63160f7843279d49463ee477fb18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ec17e5370b7442f9f34e6f7db1e13b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38f95f05972a43b6a7c6f92c4db58c27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53a49dad5c444de2929f3b9a4e4bdbd0","placeholder":"​","style":"IPY_MODEL_2ec17e5370b7442f9f34e6f7db1e13b2","value":" 2/2365 [00:42&lt;14:10:09, 21.59s/it]"}},"53a49dad5c444de2929f3b9a4e4bdbd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65c7a07f738d497eb6af4177eef3cc4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69c0a4486e0c4664921b0a715ae7b09e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a1c17a30d054d69884c5c148d7314c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2e39beddafb43bcbea678c86478f4d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_69c0a4486e0c4664921b0a715ae7b09e","max":2365,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65c7a07f738d497eb6af4177eef3cc4e","value":2}},"baabe053691e4d61b71942526720ac1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c17c53e718c34af78e37de04c171434a","placeholder":"​","style":"IPY_MODEL_15ba63160f7843279d49463ee477fb18","value":"Epoch 1/50:   0%"}},"c17c53e718c34af78e37de04c171434a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee4149431bf94221b2752786de06e872":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_baabe053691e4d61b71942526720ac1c","IPY_MODEL_b2e39beddafb43bcbea678c86478f4d8","IPY_MODEL_38f95f05972a43b6a7c6f92c4db58c27"],"layout":"IPY_MODEL_7a1c17a30d054d69884c5c148d7314c8"}}}}},"nbformat":4,"nbformat_minor":0}
