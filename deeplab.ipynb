{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloader import PASTIS_Dataset\n",
    "from collate import pad_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порядок комментариев к коду: китайский / русский / английский."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集加载 / Загрузка набора данных / loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取并处理数据集 / Получение и обработка набора данных / Getting and processing the dataset\n",
    "path_to_dataset = '/content/drive/MyDrive/Colab Notebooks/data/PASTIS'\n",
    "dataset = PASTIS_Dataset(path_to_dataset, norm=True, target='semantic') # 适用语义分割任务 / Для задач семантического сегментирования / For semantic segmentation tasks\n",
    "subset_indices = torch.randperm(len(dataset))[:2430].tolist()\n",
    "subset_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "# 划分训练集和验证集 / Разделение на обучающий и проверочный наборы / Splitting into training and validation sets\n",
    "train_size = int(0.8 * len(subset_dataset))\n",
    "valid_size = len(subset_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(subset_dataset, [train_size, valid_size])\n",
    "\n",
    "# 创建 DataLoader / Создание DataLoader / Creating DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, collate_fn=pad_collate, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, collate_fn=pad_collate)\n",
    "\n",
    "# 类别数 / Количество классов / Number of classes\n",
    "num_classes = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU / Определите, можно ли использовать Cuda / To see if Cuda can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU support enabled.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "早停 / Ранняя остановка / Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "patience: 训练过程中没有改进的次数。\n",
    "patience: Количество раз, когда обучение не улучшается.\n",
    "patience: The number of times the training does not improve.\n",
    "\n",
    "min_delta: 被认为是改进的最小变化量。\n",
    "min_delta: Минимальное изменение, которое считается улучшением.\n",
    "min_delta: The minimum change that is considered an improvement.\n",
    "\"\"\"\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减少deeplab输入通道数 / Уменьшение количества входных каналов deeplab / Reducing the number of input channels for deeplab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "由于deepLabv3模型的初始通道数是3，而数据集的通道数是10个卫星通道*30个时间点=300，所以需要修改模型第一层的通道数。\n",
    "Поскольку исходное количество каналов модели deepLabv3 равно 3, а количество каналов набора данных равно 10 каналам спутника * 30 временным точкам = 300, необходимо изменить количество каналов первого слоя модели.\n",
    "Since the initial number of channels of the deepLabv3 model is 3, and the number of channels of the dataset is 10 satellite channels * 30 time points = 300, the number of channels of the first layer of the model needs to be modified.\n",
    "\"\"\"\n",
    "def reduce_channels(model, in_channels=300):\n",
    "  deeplab_first_conv = model.backbone.conv1\n",
    "  new_first_conv = nn.Conv2d(in_channels, deeplab_first_conv.out_channels, kernel_size=deeplab_first_conv.kernel_size, stride=deeplab_first_conv.stride, padding=deeplab_first_conv.padding, bias=False)\n",
    "  model.backbone.conv1 = new_first_conv\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练  / обучение модели / model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 DeepLab 模型和优化器 / Инициализация модели DeepLab и оптимизатора / Initializing the DeepLab model and optimizer\n",
    "deeplab_model = deeplabv3_resnet50(pretrained=False, num_classes=num_classes).to(device)\n",
    "deeplab_model = reduce_channels(deeplab_model, in_channels=300)\n",
    "model = deeplab_model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, verbose=True, min_lr=1e-6)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, batch_data in tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n",
    "        ((inputs_dict, dates), targets) = batch_data\n",
    "        inputs_combined = torch.cat([inputs_dict['S2'][:, i, :, :, :] for i in range(30)], dim=1).to(device)\n",
    "        targets = targets.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs_combined)['out']\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # 验证阶段 / Валидация / Validation\n",
    "    model.eval()  # 设置模型为评估模式 / Установка модели в режим оценки / Setting the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    with torch.no_grad():  # 在这个阶段不计算梯度 / На этом этапе не вычисляются градиенты / Gradients are not calculated at this stage\n",
    "        for batch_data in valid_loader:\n",
    "            ((inputs_dict, dates), targets) = batch_data\n",
    "            inputs_combined = torch.cat([inputs_dict['S2'][:, i, :, :, :] for i in range(30)], dim=1).to(device)\n",
    "            targets = targets.to(device).long()\n",
    "\n",
    "            outputs = model(inputs_combined)['out']\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()  # 累加验证损失 / Накопление проверочной потери / Accumulating validation loss\n",
    "            # 计算准确率 / Вычисление точности / Calculating accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # 获取最大概率的预测结果 / Получение предсказанного результата с максимальной вероятностью / Getting the predicted result with the maximum probability\n",
    "            correct_pixels += (predicted == targets).sum().item()  # 累加正确预测的像素数 / Накопление количества правильно предсказанных пикселей / Accumulating the number of correctly predicted pixels\n",
    "            total_pixels += targets.nelement()  # 累加总像素数 / Накопление общего количества пикселей / Accumulating the total number of pixels\n",
    "\n",
    "    val_loss /= len(valid_loader)  # 计算平均验证损失 / Вычисление средней проверочной потери / Calculating the average validation loss\n",
    "    overall_accuracy = correct_pixels / total_pixels  # 计算总体准确率 / Вычисление общей точности / Calculating overall accuracy\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "    # 在这里调用学习率调度器，基于验证损失 / Вызов планировщика скорости обучения на основе проверочной потери / Calling the learning rate scheduler here, based on the validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 检查是否需要早停 / Проверка на необходимость досрочной остановки / Checking if early stopping is needed\n",
    "    early_stopping(loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算mIoU / Рассчитать mIoU / Calculate mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(predicted, target, num_classes):\n",
    "    \"\"\"\n",
    "    计算平均IoU，对每个类别计算IoU，然后取平均值。\n",
    "    IoU = TP / (TP + FP + FN) 指的是交集与并集的比值。\n",
    "    Вычисление среднего IoU, вычисление IoU для каждого класса, а затем взятие среднего значения.\n",
    "    IoU = TP / (TP + FP + FN) отношение пересечения к объединению.\n",
    "    Calculate mean IoU, calculate IoU for each class, then take the average.\n",
    "    IoU = TP / (TP + FP + FN) the ratio of intersection to union.\n",
    "    \"\"\"\n",
    "    iou_list = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = predicted == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "        if union == 0:\n",
    "            # 避免除以0 / Избегание деления на 0 / Avoiding division by 0\n",
    "            iou_list.append(float('nan'))  # 该类别未出现在预测和目标中 / Этот класс не появляется в прогнозе и цели / This class does not appear in the prediction and target\n",
    "        else:\n",
    "            iou_list.append(intersection / union)\n",
    "    # 忽略nan值计算平均IoU / Игнорирование значений nan при вычислении среднего IoU / Ignoring nan values when calculating the mean IoU\n",
    "    iou_list = [x for x in iou_list if not np.isnan(x)]\n",
    "    mean_iou = sum(iou_list) / len(iou_list) if iou_list else float('nan')\n",
    "    return mean_iou\n",
    "\n",
    "# 模型验证和计算Mean IoU /  Проверка модели и вычисление среднего IoU / Model validation and calculation of Mean IoU\n",
    "def validate_and_calculate_iou(model, loader, device, num_classes):\n",
    "    model.eval()\n",
    "    total_iou = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    with torch.no_grad():\n",
    "        for ((inputs_dict, dates), targets) in loader:\n",
    "            inputs_combined = torch.cat([inputs_dict['S2'][:, i, :, :, :] for i in range(30)], dim=1).to(device)\n",
    "            targets = targets.to(device).long()\n",
    "\n",
    "            outputs = model(inputs_combined)['out']\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_iou += calculate_iou(predicted, targets, num_classes)\n",
    "            # 计算准确率 / Вычисление точности / Calculating accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # 获取最大概率的预测结果 / Получение предсказанного результата с максимальной вероятностью / Getting the predicted result with the maximum probability\n",
    "            correct_pixels += (predicted == targets).sum().item()  # 累加正确预测的像素数 / Накопление количества правильно предсказанных пикселей / Accumulating the number of correctly predicted pixels\n",
    "            total_pixels += targets.nelement()  # 累加总像素数 / Накопление общего количества пикселей / Accumulating the total number of pixels\n",
    "\n",
    "    mean_iou = total_iou / len(loader)\n",
    "    overall_accuracy = correct_pixels / total_pixels  # 计算总体准确率 / Вычисление общей точности / Calculating overall accuracy\n",
    "    print(f\"Mean IoU on validation set: {mean_iou}, Overall Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证 / Проверить модель / Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用验证函数 / Вызов функции проверки / Calling the validation function\n",
    "validate_and_calculate_iou(model, valid_loader, device, num_classes)\n",
    "\n",
    "# 计算模型参数数量 / Вычисление количества параметров модели / Calculating the number of model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
